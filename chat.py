import streamlit as st
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone
from langchain_upstage import ChatUpstage
from langchain_upstage import UpstageEmbeddings
from dotenv import load_dotenv
from langsmith import Client
import os
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from llm import get_ai_response 

load_dotenv()
 
st.set_page_config(
    page_title="ê¹€ì„±í˜„ ì±—ë´‡",
    page_icon="ğŸ’©",
)

st.title("ğŸ’©ì†Œë“ì„¸ ì±—ë´‡")
st.caption("ì†Œë“ì„¸ì— ê´€í•œ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”..!!!")


if "messages_list" not in st.session_state:
    st.session_state.messages_list = []

print(f"before == {st.session_state.messages_list}")
for message in st.session_state.messages_list:
    with st.chat_message(message["role"]):
        st.write(message["content"])


if user_question := st.chat_input(placeholder="ì†Œë“ì„¸ì— ê´€í•œ ê¶ê¸ˆí•œê²ƒì„ ë¬¼ì–´ë³´ì…ˆ"):
    with st.chat_message("user"):
        st.write(user_question)
    st.session_state.messages_list.append(
        {"role": "user", "content": user_question}
    )

    with st.spinner("AIê°€ ë‹µë³€ì„ ì‘ì„±í•˜ëŠ” ì¤‘..."):
        ai_response = get_ai_response(user_question)
        with st.chat_message("ai"):
            ai_message = st.write_stream(ai_response)
        st.session_state.messages_list.append(
            {"role": "ai", "content": ai_message}
        )

print(f"after == {st.session_state.messages_list}")
